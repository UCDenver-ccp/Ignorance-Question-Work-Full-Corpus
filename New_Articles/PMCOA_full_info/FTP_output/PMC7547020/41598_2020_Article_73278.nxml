<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">33037266</article-id><article-id pub-id-type="pmc">7547020</article-id><article-id pub-id-type="publisher-id">73278</article-id><article-id pub-id-type="doi">10.1038/s41598-020-73278-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Robust deep learning classification of adamantinomatous craniopharyngioma from limited preoperative radiographic images</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Prince</surname><given-names>Eric W.</given-names></name><address><email>Eric.Prince@CUAnschutz.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Whelan</surname><given-names>Ros</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Mirsky</surname><given-names>David M.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Stence</surname><given-names>Nicholas</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Staulcup</surname><given-names>Susan</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Klimo</surname><given-names>Paul</given-names></name><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Anderson</surname><given-names>Richard C. E.</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>Niazi</surname><given-names>Toba N.</given-names></name><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author"><name><surname>Grant</surname><given-names>Gerald</given-names></name><xref ref-type="aff" rid="Aff9">9</xref></contrib><contrib contrib-type="author"><name><surname>Souweidane</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="Aff10">10</xref><xref ref-type="aff" rid="Aff11">11</xref></contrib><contrib contrib-type="author"><name><surname>Johnston</surname><given-names>James M.</given-names></name><xref ref-type="aff" rid="Aff12">12</xref></contrib><contrib contrib-type="author"><name><surname>Jackson</surname><given-names>Eric M.</given-names></name><xref ref-type="aff" rid="Aff13">13</xref></contrib><contrib contrib-type="author"><name><surname>Limbrick</surname><given-names>David D.</given-names><suffix>Jr.</suffix></name><xref ref-type="aff" rid="Aff14">14</xref></contrib><contrib contrib-type="author"><name><surname>Smith</surname><given-names>Amy</given-names></name><xref ref-type="aff" rid="Aff15">15</xref></contrib><contrib contrib-type="author"><name><surname>Drapeau</surname><given-names>Annie</given-names></name><xref ref-type="aff" rid="Aff16">16</xref></contrib><contrib contrib-type="author"><name><surname>Chern</surname><given-names>Joshua J.</given-names></name><xref ref-type="aff" rid="Aff17">17</xref></contrib><contrib contrib-type="author"><name><surname>Kilburn</surname><given-names>Lindsay</given-names></name><xref ref-type="aff" rid="Aff18">18</xref></contrib><contrib contrib-type="author"><name><surname>Ginn</surname><given-names>Kevin</given-names></name><xref ref-type="aff" rid="Aff19">19</xref></contrib><contrib contrib-type="author"><name><surname>Naftel</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="Aff20">20</xref></contrib><contrib contrib-type="author"><name><surname>Dudley</surname><given-names>Roy</given-names></name><xref ref-type="aff" rid="Aff21">21</xref></contrib><contrib contrib-type="author"><name><surname>Tyler-Kabara</surname><given-names>Elizabeth</given-names></name><xref ref-type="aff" rid="Aff22">22</xref></contrib><contrib contrib-type="author"><name><surname>Jallo</surname><given-names>George</given-names></name><xref ref-type="aff" rid="Aff23">23</xref></contrib><contrib contrib-type="author"><name><surname>Handler</surname><given-names>Michael H.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Jones</surname><given-names>Kenneth</given-names></name><xref ref-type="aff" rid="Aff24">24</xref></contrib><contrib contrib-type="author"><name><surname>Donson</surname><given-names>Andrew M.</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff25">25</xref></contrib><contrib contrib-type="author"><name><surname>Foreman</surname><given-names>Nicholas K.</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff25">25</xref></contrib><contrib contrib-type="author"><name><surname>Hankinson</surname><given-names>Todd C.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.413957.d</institution-id><institution-id institution-id-type="ISNI">0000 0001 0690 7621</institution-id><institution>Division of Pediatric Neurosurgery, </institution><institution>Children&#x02019;s Hospital Colorado, </institution></institution-wrap>Aurora, 80045 USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.430503.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0703 675X</institution-id><institution>Department of Neurosurgery, </institution><institution>University of Colorado School of Medicine, </institution></institution-wrap>Aurora, 80045 USA </aff><aff id="Aff3"><label>3</label>Morgan Adams Foundation Pediatric Brain Tumor Research Program, Aurora, 80045 USA </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.413957.d</institution-id><institution-id institution-id-type="ISNI">0000 0001 0690 7621</institution-id><institution>Division of Pediatric Radiology, </institution><institution>Children&#x02019;s Hospital Colorado, </institution></institution-wrap>Aurora, 80045 USA </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.267301.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0386 9246</institution-id><institution>Department of Neurosurgery, </institution><institution>University of Tennessee Health and Sciences Center, </institution></institution-wrap>Memphis, 38163 USA </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.240871.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 0224 711X</institution-id><institution>Semmes Murphy Clinic, </institution><institution>St. Jude Children&#x02019;s Research Hospital, </institution></institution-wrap>Memphis, 38105 USA </aff><aff id="Aff7"><label>7</label>Neurosurgical Associates of New Jersey, Ridgewood, NJ 07450 USA </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.415486.a</institution-id><institution-id institution-id-type="ISNI">0000 0000 9682 6720</institution-id><institution>Department of Pediatric Neurosurgery, </institution><institution>Nicklaus Children&#x02019;s Hospital, </institution></institution-wrap>Miami, 33155 USA </aff><aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.414123.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0450 875X</institution-id><institution>Department of Pediatric Neurosurgery, </institution><institution>Lucile Packard Children&#x02019;s Hospital at Stanford University, </institution></institution-wrap>Palo Alto, 94305 USA </aff><aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="GRID">grid.51462.34</institution-id><institution-id institution-id-type="ISNI">0000 0001 2171 9952</institution-id><institution>Department of Neurosurgery, </institution><institution>Memorial Sloan Kettering Cancer Center, </institution></institution-wrap>New York, 10065 USA </aff><aff id="Aff11"><label>11</label><institution-wrap><institution-id institution-id-type="GRID">grid.5386.8</institution-id><institution-id institution-id-type="ISNI">000000041936877X</institution-id><institution>Department of Neurological Surgery, </institution><institution>Weill Cornell Medical College, </institution></institution-wrap>New York, 10065 USA </aff><aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="GRID">grid.265892.2</institution-id><institution-id institution-id-type="ISNI">0000000106344187</institution-id><institution>Division of Pediatric Neurosurgery, </institution><institution>University of Alabama at Birmingham, </institution></institution-wrap>Birmingham, 35233 USA </aff><aff id="Aff13"><label>13</label><institution-wrap><institution-id institution-id-type="GRID">grid.21107.35</institution-id><institution-id institution-id-type="ISNI">0000 0001 2171 9311</institution-id><institution>Department of Neurosurgery, </institution><institution>Johns Hopkins University School of Medicine, </institution></institution-wrap>Baltimore, 21205 USA </aff><aff id="Aff14"><label>14</label><institution-wrap><institution-id institution-id-type="GRID">grid.4367.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2355 7002</institution-id><institution>Department of Pediatrics, </institution><institution>Washington University School of Medicine, </institution></institution-wrap>St. Louis, 63110 USA </aff><aff id="Aff15"><label>15</label><institution-wrap><institution-id institution-id-type="GRID">grid.413939.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 0456 3548</institution-id><institution>Department of Pediatric Hematology-Oncology, </institution><institution>Arnold Palmer Hospital, </institution></institution-wrap>Orlando, 32806 USA </aff><aff id="Aff16"><label>16</label><institution-wrap><institution-id institution-id-type="GRID">grid.240344.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 0392 3476</institution-id><institution>Division of Pediatric Neurosurgery, </institution><institution>Nationwide Children&#x02019;s Hospital, </institution></institution-wrap>Columbus, 43205 USA </aff><aff id="Aff17"><label>17</label><institution-wrap><institution-id institution-id-type="GRID">grid.189967.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 0941 6502</institution-id><institution>Departments of Pediatrics and Neurosurgery, </institution><institution>Emory University School of Medicine, </institution></institution-wrap>Atlanta, 30322 USA </aff><aff id="Aff18"><label>18</label><institution-wrap><institution-id institution-id-type="GRID">grid.239560.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 0482 1586</institution-id><institution>Children&#x02019;s National Health System, Brain Tumor Institute, </institution></institution-wrap>Washington, DC 20010 USA </aff><aff id="Aff19"><label>19</label><institution-wrap><institution-id institution-id-type="GRID">grid.239559.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0415 5050</institution-id><institution>Division of Pediatric Hematology and Oncology, </institution><institution>Children&#x02019;s Mercy Hospital, </institution></institution-wrap>Kansas City, 64108 USA </aff><aff id="Aff20"><label>20</label><institution-wrap><institution-id institution-id-type="GRID">grid.416074.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 0433 6783</institution-id><institution>Department of Neurological Surgery, </institution><institution>Monroe Carell Jr. Children&#x02019;s Hospital at Vanderbilt, </institution></institution-wrap>Nashville, 37212 USA </aff><aff id="Aff21"><label>21</label><institution-wrap><institution-id institution-id-type="GRID">grid.14709.3b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8649</institution-id><institution>Department of Neurosurgery, </institution><institution>McGill University, </institution></institution-wrap>Montreal, H3A 2B4 Canada </aff><aff id="Aff22"><label>22</label><institution-wrap><institution-id institution-id-type="GRID">grid.21925.3d</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9000</institution-id><institution>Department of Neurological Surgery, </institution><institution>University of Pittsburgh, </institution></institution-wrap>Pittsburgh, 15213 USA </aff><aff id="Aff23"><label>23</label><institution-wrap><institution-id institution-id-type="GRID">grid.413611.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 0467 2330</institution-id><institution>Institute of Brain Protection Sciences, </institution><institution>Johns Hopkins All Children&#x02019;s Hospital, </institution></institution-wrap>St Petersburg, 33701 USA </aff><aff id="Aff24"><label>24</label><institution-wrap><institution-id institution-id-type="GRID">grid.266902.9</institution-id><institution-id institution-id-type="ISNI">0000 0001 2179 3618</institution-id><institution>University of Oklahoma Health Sciences Center, </institution></institution-wrap>Oklahoma City, 73104 USA </aff><aff id="Aff25"><label>25</label><institution-wrap><institution-id institution-id-type="GRID">grid.413957.d</institution-id><institution-id institution-id-type="ISNI">0000 0001 0690 7621</institution-id><institution>Division of Pediatric Neurooncology, </institution><institution>Children&#x02019;s Hospital Colorado, </institution></institution-wrap>Aurora, 80045 USA </aff></contrib-group><pub-date pub-type="epub"><day>9</day><month>10</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>9</day><month>10</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>10</volume><elocation-id>16885</elocation-id><history><date date-type="received"><day>10</day><month>2</month><year>2020</year></date><date date-type="accepted"><day>25</day><month>8</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Deep learning (DL) is a widely applied mathematical modeling technique. Classically, DL models utilize large volumes of training data, which are not available in many healthcare contexts. For patients with brain tumors, non-invasive diagnosis would represent a substantial clinical advance, potentially sparing patients from the risks associated with surgical intervention on the brain. Such an approach will depend upon highly accurate models built using the limited datasets that are available. Herein, we present a novel genetic algorithm (GA) that identifies optimal architecture parameters using feature embeddings from state-of-the-art image classification networks to identify the pediatric brain tumor, adamantinomatous craniopharyngioma (ACP). We optimized classification models for preoperative Computed Tomography (CT), Magnetic Resonance Imaging (MRI), and combined CT and MRI datasets with demonstrated test accuracies of 85.3%, 83.3%, and 87.8%, respectively. Notably, our GA improved baseline model performance by up to 38%. This work advances DL and its applications within healthcare by identifying optimized networks in small-scale data contexts. The proposed system is easily implementable and scalable for non-invasive computer-aided diagnosis, even for uncommon diseases.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Paediatric cancer</kwd><kwd>Machine learning</kwd><kwd>Cancer imaging</kwd></kwd-group><funding-group><award-group><funding-source><institution>University of Colorado Comprehensive Cancer Center</institution></funding-source><award-id>P30CA046934</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2020</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Deep learning is a subtype of artificial intelligence that constructs generalizable models for data representations via a multilayer abstraction process<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. A common deep learning architecture used for classification of visual information is known as a Convolutional Neural Network (CNN). CNNs are constructed using multiple sequential layers containing variants of the multi-layer perceptron. These networks have demonstrated generalization capacity for identifying both linear and non-linear latent patterns that lead to differentiable information<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. CNNs and other variants have had great success in tasks such as image object recognition; speech recognition, translation, and generation; and medical diagnostics, genetics, and drug discovery<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. These applications have achieved remarkable success, to some extent by leveraging very large amounts of labeled training data. An example is the ImageNet Large Scale Visual Recognition Challenge (ILVSRC). This leading image recognition competition challenges competitors to advance the state of the art in computer-guided object detection and classification. Using the ImageNet dataset, comprising over 1.4 million images across more than 1000 possible categories, CNNs are achieving error rates under 5%<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.
</p><p id="Par3">Within the healthcare space, reliable CNN inference models have been described under conditions when vast amounts of training data are available. Examples include dermatological diseases and diabetic retinopathy<sup><xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref></sup>. However, when such models are trained on more limited datasets, the results are often unreliable, as the models overfit the training data. More specifically, in a small-data context, the latent features that a network models are likely to result from sampling noise that exists only in the training data, and not in novel test data<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Without techniques to overcome this generalization problem, CNNs may have limited applications for less common diseases, including brain tumors.</p><p id="Par4">One technique available to overcome the overfitting complication of small training datasets is Transfer Learning (TL). This is a machine learning methodology for storing knowledge gained from solving a problem within one domain and applying that knowledge to another domain<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>. The success of TL has led to the development of publicly available pre-trained models derived from top ILSVRC solutions. By using these pre-trained networks to generate feature embeddings for our dataset of interest, we enable our classifier to have access to the pattern recognition capabilities of these state-of-the-art architectures.</p><p id="Par5">Another technique commonly applied to image classification problems is data augmentation. This process synthetically expands a dataset by applying transformations (i.e. crop, rotate, blur, etc.) to real data in an attempt to preserve domain-specific features. We employed two separate data augmentation approaches. The first was a stochastic process that sampled across transformations with probability thresholds. The second method, known as TANDA (Transformation Adversarial Networks for Data Augmentation), is a ML-based approach that uses Generative Adversarial Networks (GANs) and Recurrent Neural Network (RNNs) to learn the optimal combination and parameters of the image transformations within a specific dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. TANDA was reported to yield synthetic data in which feature representations are distributed and invariant, thus helping disentangle the factors of variation between the two classes<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p><p id="Par6">An additional challenge in identifying the optimal model is the optimization of CNN hyperparameters. This remains a complicated and computationally intensive task<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. To mitigate the computational time required, one may apply a meta-heuristic parameter optimization in the form of an asynchronously parallelized genetic algorithm. This optimization procedure allows the model to optimize more intelligently over the solution space with fewer required iterations.</p><p id="Par7">To demonstrate the capacity of combining deep networks, transfer learning, data augmentation, and genetic algorithms to overcome the problem of overfitting with small datasets, we utilized the pediatric brain tumor Adamantinomatous Craniopharyngioma (ACP).</p><p id="Par8">ACP is a neurologically devastating brain tumor that is notorious for causing vision loss, hypothalamic injury, hormone dysfunction and cerebrospinal fluid pathway obstruction, among other injuries. This damage results from growth of the tumor in the sellar/suprasellar region of the brain, where it invariably develops. Here, ACP compresses the optic apparatus, hypothalamic-pituitary axis, and cerebral ventricular system. While ACP is a histologically benign lesion, it often recurs locally, which makes further treatment more perilous for the patient. As such, ACP has been associated with the lowest quality of life scores of any pediatric brain tumor<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Current therapeutic management of ACP is limited to either aggressive surgical resection or surgical debulking followed by external beam radiation. This differs considerably from the therapy for other tumors that present in the sellar/suprasellar region. For example, Germinoma, one of the most common tumors in the radiographic differential diagnosis of ACP, is effectively treated without surgical intervention. Other masses of this region, including glioma, pituitary adenoma, arachnoid cysts, and others, similarly require therapy tailored to the particular entity. As such, a priori knowledge of the patients diagnosis would considerably improve the clinical care of children with tumors of the sellar/suprasellar region, the most common of which is ACP.</p><p id="Par9">Radiographically, ACP is characterized by heterogeneous solid tissue, cystic regions, and calcification<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Recent data indicate that ACP and other tumors of the sellar/suprasellar region may be accurately diagnosed using current radiographic techniques in 64&#x02013;87%<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> of cases. This sets a high bar for machine-aided diagnoses, but also leaves room for clinically relevant improvement.</p><p id="Par10">While ACP is the most common sellar/suprasellar pediatric tumor, it is an uncommon entity, representing 2 to 5% of all pediatric brain tumors, with an incidence of approximately 1.9 per million patient-years<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. In order to facilitate research into this tumor, Advancing Treatment for Pediatric Craniopharyngioma was formed in 2015. This consortium includes 17 North American centers, which share tissue and clinical data regarding children with ACP, thus providing source data for this research. In addition to the imaging data assembled from these centers, we added data from St. Jude Children&#x02019;s Research Hospital, thereby assembling a generalizable and representative dataset of both ACP and other sellar/suprasellar entities for model training and evaluation.</p><p id="Par11">In summary, ACP is an ideal candidate for CNN inference due to its consistent anatomical location, radiographically recognizable features, and, most importantly, the substantial clinical management differences between ACP and the other brain masses that lie within the differential diagnosis. However, given its incidence, ACP lacks the volume of labeled data observed in more common disease contexts. By describing a mathematical model for the identification of ACP, we present a computationally economical method to optimize CNN architectures for image classification in contexts that do not afford large amounts of labeled training data. In so doing, we create a non-invasive diagnostic tool to aid in the reduction of mis-diagnoses and unnecessary medical intervention.</p></sec><sec id="Sec2"><title>Results</title><sec id="Sec3"><title>Baseline predictive results</title><p id="Par12">Using twelve state-of-the-art networks that have publicly available deep learning models from the TensorFlow Hub library<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> trained on the ImageNet ILSVRC dataset (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>A)<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR12">12</xref>&#x02013;<xref ref-type="bibr" rid="CR17">17</xref></sup>, we generated feature embedding vectors to be used in model training (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>B). Baseline experiments were conducted by training a single fully-connected layer with a softmax activation function and stochastic gradient descent (SGD) optimization algorithm. Using whole-batch training, a learning rate of 0.01, and a training duration of 100 epochs, we established baseline results (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>C). Across all twelve feature embeddings, on average the classifier accurately labelled individual CT scans 62.3% (Top-5 Network Average <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$({\hbox {T5NA}}) = 73.3\%$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>T5NA</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>73.3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq1.gif"/></alternatives></inline-formula>; maximum performance <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$({\hbox {MP}}) = 80.0\%$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>MP</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>80.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq2.gif"/></alternatives></inline-formula>) and MRI scans 45.7% (<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {T5NA}} = 54.0\%$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mtext>T5NA</mml:mtext><mml:mo>=</mml:mo><mml:mn>54.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq3.gif"/></alternatives></inline-formula>; <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {MP}} = 64.7\%$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mtext>MP</mml:mtext><mml:mo>=</mml:mo><mml:mn>64.7</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq4.gif"/></alternatives></inline-formula>) of the time.<fig id="Fig1"><label>Figure 1</label><caption><p>Transfer learning networks, feature embeddings, and baseline results. (<bold>A</bold>) ILSVRC network models utilized, with their top 1% and top 5% accuracy in ILSVRC competition noted. (<bold>B</bold>) Example CT and MRI images for both ACP and NOTACP. (<bold>C</bold>) ROC (left) and AUC (right) values for all twelve networks and both imaging modalities (CT top, MRI bottom). The diagonal dashed line represents performance of a random guess.</p></caption><graphic xlink:href="41598_2020_73278_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec4"><title>Model selection and parameter optimization using manual selection</title><p id="Par13">To address the computational and time demands associated with architecture selection and hyperparameter optimization within deep learning models<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, we employed a parallelized simple genetic algorithm (GA) to more rapidly identify optimal combinations of feature extractors, learning parameters, and hyperparameters for both CT and MRI (see Computational Methods; Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>A).<fig id="Fig2"><label>Figure 2</label><caption><p>Genetic algorithm optimization of model parameters. (<bold>A</bold>) General process schematic for genetic algorithm parameter optimization. Moving from left to right, a feature variant is selected for each model feature to create individual networks (Step 1; individuals are highlighted in unique colors). Individuals are trained and evaluated to determine fitness and ranked accordingly (Step 2). Two networks are chosen from the fittest population and a new network is derived by selecting from feature variants in these two networks, and variants are occasionally mutated (i.e., randomly selected from the population pool; Step 3). (<bold>B</bold>) Model feature and respective feature variants explored in first phase of genetic algorithm optimization. Each column represents a model feature to be optimized and each row is a possible feature variant for the GA to select from. This table reflects the &#x0201c;Population Pool&#x0201d; (<bold>A</bold>). (<bold>C</bold>) Top-5 performing networks for independent CT and MRI networks after 10 generations of 100 solution populations; ranked according to test accuracy. (<bold>D</bold>) Top-5 performing networks for combined CT-MRI networks after 10 generations of 100 solution populations; ranked according to test accuracy.</p></caption><graphic xlink:href="41598_2020_73278_Fig2_HTML" id="MO2"/></fig></p><p id="Par14">We employed ten different model features to optimize the parameters of the (1) fully connected classifier network; (2) the pre-trained deep CNN to be utilized for feature embeddings; and, (3) the type of training and test datasets (e.g. original or synthetically expanded by augmentation; Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>B). The number of variants for each model feature ranged from three to fifteen, making the total number of possible network combinations 19,051,200 (see "<xref rid="Sec10" ref-type="sec">Methods</xref>" section). The GA allowed for more intelligent exploration of the solution space and reduced the overall computational time required. By performing 10 generations of 100 solution populations with a generational retention rate of 40%, a negative rejection probability of 10%, and a mutation frequency of 20%, we explored only <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 10^3$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq5.gif"/></alternatives></inline-formula> solutions to identify the top 5 performing networks of the final generation (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>C). This process yielded an accuracy increase in CT of only 3.75% (<inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {T5NA}} = 82.3\%$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mtext>T5NA</mml:mtext><mml:mo>=</mml:mo><mml:mn>82.3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq6.gif"/></alternatives></inline-formula>; <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {MP}} = 83.8\%$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mtext>MP</mml:mtext><mml:mo>=</mml:mo><mml:mn>83.8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq7.gif"/></alternatives></inline-formula>) and an an increase of 16.8% (<inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {T5NA}} = 80.3\%$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mtext>T5NA</mml:mtext><mml:mo>=</mml:mo><mml:mn>80.3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq8.gif"/></alternatives></inline-formula>; <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {MP} }= 83.3\%$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mtext>MP</mml:mtext><mml:mo>=</mml:mo><mml:mn>83.3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq9.gif"/></alternatives></inline-formula>) for MRI.</p></sec><sec id="Sec5"><title>Model selection and parameter optimization using a simple genetic algorithm</title><p id="Par15">When the genetic algorithm was employed, a superior network became apparent for both CT- and MRI-trained classifiers. Interestingly, the same general architecture&#x02014;ResNet&#x02014;was selected, with CT data favoring the V1 variant and MRI favoring the V2 variant. The primary difference between these architectures is the use of batch normalization (BN) between every layer in V2 as opposed to V1. The BN transformation is particularly resilient toward parameter scale because backpropagation through a layer is unaffected by the scale of its parameters<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. This suggests that MRI data contained more erratic feature distributions than CT and therefore benefited from the more regularized representation. Further, the superiority of each respective network is highlighted by their selection within the combined CT-MRI network (<inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {T5NA}} = 81.7\%; {\hbox {MP}} = 85.4\%$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mtext>T5NA</mml:mtext><mml:mo>=</mml:mo><mml:mn>81.7</mml:mn><mml:mo>%</mml:mo><mml:mo>&#x0037e;</mml:mo><mml:mtext>MP</mml:mtext><mml:mo>=</mml:mo><mml:mn>85.4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq10.gif"/></alternatives></inline-formula>; Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>D).</p><p id="Par16">While performance improved within both modalities compared to the pre-GA results, this method searched only 0.005% of the total number of solutions. We sought to verify that we did not identify a local minima within the model solution space by exploring 0.1% of all solutions. In every GA iteration all paramaters were equally distributed in the first generation, but by the last generation the same end-point was reached with similar results via similar &#x0201c;evolutionary&#x0201d; paths to those presented in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> (data not shown). To further improve model generalization, we performed a second iteration of the GA with fewer parameters (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>A). In this iteration, we allowed the GA to evolve a parameter population with only 144 possible combinations for CT and 1944 possible solutions for MRI. After running the GA on the new refined feature lists for 10 generations with 100 solutions per generation, the top accuracy for CT increased by a further 1.54% (T5NA&#x000a0;=&#x000a0;83.4%; MP&#x000a0;=&#x000a0;85.3%; Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>B). Similarly, we observed an increase in performance for CT-MRI networks (T5NA = 86.1%; MP = 87.8%; Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>C). Interestingly, however, the solution networks for MRI classification did not attain the same level of accuracy as the initial GA iteration (T5NA = 78.5%; MP = 80.8%; Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>B). This could be due to our optimization algorithm erroneously identifying a local minimum of the solution space, as opposed to the desired global minimum.<fig id="Fig3"><label>Figure 3</label><caption><p>Further optimization with GA. (<bold>A</bold>). Model features and variants available for solution search space in second phase of GA optimization. Each column represents a model feature to be optimized and each row is a possible feature variant for the GA to select from. (<bold>B</bold>) Top-5 performing networks from CT and MRI trained networks as optimized by the GA for 10 generations of 100 solution populations. (<bold>C</bold>) Top-5 performing networks for combined CT-MRI trained networks as optimized by the GA for 10 generations of 100 solution populations.</p></caption><graphic xlink:href="41598_2020_73278_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec6"><title>Stochastically augmented training data outperforms TANDA augmented training data</title><p id="Par17">It is well documented that data augmentation improves performance of state-of-the-art image classification models<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. Two potential data augmentation solutions are the stochastic-based and ML-based augmentation pipelines. We explored both solutions, using the publicly available python modules Augmentor (stochastic-based) and TANDA (LSTM-GAN-based) to oversample our training data to 1000 images per class (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>A). Interestingly, optimal models trained on stochastically augmented data outperformed those same architectures trained on TANDA augmented data (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>B).<fig id="Fig4"><label>Figure 4</label><caption><p>Test performance of models trained on stochastic image augmentation and GAN-LSTM augmented images. (<bold>A</bold>) Exemplar images of original training CT (top) and MRI (bottom), with randomly augmented variants, and TANDA-augmented variants. (<bold>B</bold>) ROC curves for CT- and MRI-trained networks comparing top results of supervised augmented images and TANDA-generated images. Dashed lines represent ROC curve of random chance classification.</p></caption><graphic xlink:href="41598_2020_73278_Fig4_HTML" id="MO4"/></fig></p><p id="Par18">This finding could result from our use of TANDA with parameters given for the MNIST handwritten-image dataset context, rather than applying TANDA following optimization for our specific context. While this represents an area for additional investigation, current literature suggests that TANDA can be expected to realize improvements of 2&#x02013;4% in CNNs with architectures comparable to our own<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Given this fairly small improvement, there exists a computational efficiency argument that favors the use of the stochastic process, especially in early stage investigations, or in contexts where computational resources are limited. In our study, the stochastic approach yielded superior results without the requirement to train and evaluate complex machine-learning systems and with lesser computational demand.</p><p id="Par19">An additional aspect of our data that could favor the use of stochastic data augmentation is the relative conspicuity of the critical features of the image. Both due to the nature of a tumor being a mass lesion, and the quality of current medical imaging technologies, the primary source of differentiable information in the images that compose our dataset lies within the sellar/suprasellar region (discussed in the following section), with a gradient of decreasing value as one moves radially away from this region. The resultant relative simplicity in the images may therefore lead to only a marginal difference between stochastic augmentation and TANDA. Datasets in which the target object is more difficult to distinguish from the background (for example, identifying a person wearing black and white stripes among a group of zebras) may, however, better demonstrate the advantage of the more complex TANDA methodology.</p></sec><sec id="Sec7"><title>Manual objective obfuscation indicates the sellar/suprasellar region is critical to class identification</title><p id="Par20">To understand the general patterns the model identified as class indicators, we performed manual objective obfuscation of the sellar/suprasellar region in all training images (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>A). The previously identified optimal networks were trained on these obfuscated data and subsequently used to infer diagnosis from the test set. In this context, the networks failed to accurately distinguish ACP from NOTACP images (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>B). Interestingly, however, when the baseline networks were trained using obfuscated data, some networks reliably distinguished data classes (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>C,D). This suggests that while the GA-identified networks utilize image patterns within the sellar/suprasellar region, other non-optimized networks identify latent patterns outside of the sellar/suprasellar region, which is the anatomical location of ACP. As such, a potential improvement to our model could be to integrate feature embeddings from all networks in order to leverage both sellar/suprasellar and extra-sellar patterns within the data.<fig id="Fig5"><label>Figure 5</label><caption><p>Pituitary obfuscation reveals latent features exist outside canonical ROI for CT scans. (<bold>A</bold>) Example original and obfuscated images for both data classes and both imaging modalities. (<bold>B</bold>) ROC curves for networks trained on obfuscated and original data; original data was &#x02018;Augmented (<inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {N}}=1000$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mtext>N</mml:mtext><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq11.gif"/></alternatives></inline-formula>)&#x02019; variant. (<bold>C</bold>) Baseline ROC curves for all twelve networks trained on original (left) and obfuscated (right) CT images. (<bold>D</bold>) Baseline ROC curves for all twelve networks trained on original (left) and obfuscated (right) MRI images.</p></caption><graphic xlink:href="41598_2020_73278_Fig5_HTML" id="MO5"/></fig></p></sec><sec id="Sec8"><title>Benchmarking against human performance and assessment of hold out training/testing approach</title><p id="Par21">Next, we sought to compare the generalization capacity of our GA-optimized models against the performance of board-certified pediatric neuroradiologists. Using the same test dataset (1 JPEG image per modality per unique patient) used to determine &#x02018;fitness&#x02019; within the GA, two specialists were asked to classify diagnosis of ACP/NOTACP in a binary context (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>). Our optimal models performed on par with the average of human specialists (<inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}=0.39$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.39</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq12.gif"/></alternatives></inline-formula>), although &#x02018;Radiologist A&#x02019; consistently outperformed our models across the board. As mentioned previously, recent work reported an accuracy of ACP diagnosis of 87% by pediatric neuroradiologists using a complete imaging dataset and clinical history<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. This performance corroborates the overall generalization capacity of the models presented herein.<fig id="Fig6"><label>Figure 6</label><caption><p>Optimized network classification performance versus human specialist and 5-fold cross-validation evaluation. (<bold>A</bold>) Radiologist average auROC of 89.4%, 83.3%, and 93.8% for CT, MRI, and CT-MRI, respectively. GA-optimzed auROC of 85.3%, 83.3%, and 87.8% for CT, MRI, and CT-MRI, respectively. (<bold>B</bold>) Schematics of 5-fold cross-validation (5F-CV) approaches used to verify the perceived improvement yielded by augmented training data (scenario 3 vs. scenarios 1 and 2). Additionally scenarios 1 and 2 investigate the effect of mixing augmented data into the overall data pool versus only augmenting training data. (<bold>C</bold>) Performance metrics (AUC: area under the ROC curve; Accuracy: standard accuracy metric) for 5F-CV across all three scenarios. Peak performance was achieved via scenario 2 in CT (AUC&#x000a0;=&#x000a0;88.0%, Accuracy&#x000a0;=&#x000a0;89.0%) and MRI (AUC&#x000a0;=&#x000a0;97.5%, Accuracy&#x000a0;=&#x000a0;97.4%). In the context of CT-MRI, peak performance was attained in scenario 3 (AUC&#x000a0;=&#x000a0;97.8%, Accuracy&#x000a0;=&#x000a0;97.9%).</p></caption><graphic xlink:href="41598_2020_73278_Fig6_HTML" id="MO6"/></fig></p><p id="Par22">Since the dataset utilized is small and therefore sensitive to selection bias, there is concern that the hold-out approach may misrepresent the true predictive capacity of our classifier. To address this, we additionally evaluated our classifier using five-fold cross-validation (5F-CV). Our GA previously identified that augmented data were ideal for training and original data were ideal for test data, thus we chose to evaluate 5F-CV in three different scenarios to verify that augmenting training data are superior (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>). To further verify the power of our classifier, we chose two separate approaches as to when data is augmented. Interestingly, the 5F-CV data suggests model performance greater than results yielded in the hold-out approach (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>C). We see that for CT and MRI contexts, augmenting only training data yields the highest results (88.9% and 97.3%, respectively; Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>B. Scenario 2) and classifiers trained on original data perform worse, as expected. This is particularly interesting due to the expectation that mixing augmented data (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>B. Scenario 1) should increase data overlap and therefore lead to overfitting and inflated performance metrics. Lastly, the CT-MRI context demonstrated that peak performance was attained using original training and test data. Since the CT-MRI context concatenates feature embeddings along a 1-dimensional axis, perhaps these features contain sufficient classification information without the need for augmentation. In fact, information needed in the CT-MRI context may be obscured or dropped by concatenating two augmented case examples. In summary, the combination of hold-out and 5F-CV performance metrics firmly verifies the robustness of our GA-based approach and our resultant classifiers.</p></sec></sec><sec id="Sec9"><title>Discussion</title><p id="Par23">Using the pediatric brain tumor Adamantinomatous Craniopharyngioma as an example of a clinical entity with a small available dataset, we enhance the performance of a baseline Convolutional Neural Network using a series of optimization methodologies, including Transfer Learning, Data Augmentation (supervised and unsupervised), and Image Obfuscation. The application of a Genetic Algorithm as a meta-heuristic optimizer realized performance improvements of approximately 23% for CT-trained networks, and 38% for MRI-trained networks, leading to test accuracies of 85.3%, 83.3%, and 87.8% for Computed Tomography (CT), Magnetic Resonance Imaging (MRI), and combined CT and MRI datasets, respectively. We further demonstrate that this is equivalent to the diagnostic accuracy of clinical experts (<inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}=0.39$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.39</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq13.gif"/></alternatives></inline-formula>). Lastly, we verified the results of the hold-out test set approach we utilized by demonstrating increased performance under the auspices of 5-fold cross-validation.</p><p id="Par24">Notably, in line with human performance, combining CT and MRI together resulted in higher performance across the board. This is likely due to the increase in relevant information put forth by consideration of both imaging modalities. Furthermore, it is also interesting that we see the baseline performance of CT being very close to the optimized performance in contrast to the larger performance improvement seen in the context of MRI. This is likely due to the intrinsic differences between each classification problem. Meaning, that the pre-trained network feature (as opposed to learning rate, batch size, regularization, etc.) in the CT scenario had the relative greatest impact on overall model performance. Through this kind of perspective it is possible to utilize the GA to extract feature importance information from the GA search space. Additionally, the asynchronous parallelization of our optimization algorithm increased efficiency both in terms of the number of solutions to consider as well as the computational time and resources required to complete calculations. This offers evidence that these techniques may be broadly applied to the development of other parameterized machine learning models in the context of limited training data.</p><p id="Par25">As this work represents an initial exploration of these methodologies, the presented model may be improved. For example, it is possible that the TANDA algorithm could itself be optimized by a GA or other meta-heuristic algorithm, such as particle swarm optimization. Another possible improvement could be to aggregate feature embeddings from all networks as input data for each real image, thus synthetically expanding the dataset in a manner that leverages pre-trained feature extraction. Lastly, we explored only one type of classifier. Other classifiers, such as a Random Forest-based method or a deeper classifier, while possibly more prone to overfitting, may also improve performance.</p></sec><sec id="Sec10"><title>Methods</title><sec id="Sec11"><title>Image acquisition</title><p id="Par26">Deidentified preoperative DICOM image sets for 39 unique patients with histologically confirmed ACP were obtained through the Advancing Treatment for Pediatric Craniopharyngioma consortium (<inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {n}}=34$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>34</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq14.gif"/></alternatives></inline-formula>) and the St. Jude Children&#x02019;s Research Hospital (<inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hbox {n}}=5$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq15.gif"/></alternatives></inline-formula>). Per the Colorado Multiple Institutional Review Board and United States Health and Human Services Regulation 45 CFR 46, this study was exempt from requiring Institutional Review Board approval. Where otherwise concerned, appropriate informed consent was obtained in accordance with the Declaration of Helsinki (v. 2013). Sagittal T1-weighted MRI and axial non-contrast CT image series were selected, based on the fact that the 2 modalities are used in a complementary manner in the clinical setting. A board-certified pediatric neurosurgeon selected individual images from each series, based on their demonstration of the disease process. These were exported as <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$299\times 299$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mn>299</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>299</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq16.gif"/></alternatives></inline-formula> pixel JPEG images. This procedure was also performed on analogous imaging studies from 47 unique patients with histologically confirmed non-ACP sellar/suprasellar lesions (NOTACP), which were in the radiological differential diagnosis of ACP. These included pilocytic astrocytoma (<inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=12$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq17.gif"/></alternatives></inline-formula>), germinoma (<inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=7$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq18.gif"/></alternatives></inline-formula>), pilomixoid astrocytoma (<inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=6$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq19.gif"/></alternatives></inline-formula>), optic glioma (<inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=4$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq20.gif"/></alternatives></inline-formula>), pituitary adenoma (<inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=3$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq21.gif"/></alternatives></inline-formula>), arachnoid cyst (<inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=3$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq22.gif"/></alternatives></inline-formula>), prolactinoma (<inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=3$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq23.gif"/></alternatives></inline-formula>), mature teratoma (<inline-formula id="IEq24"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=2$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq24.gif"/></alternatives></inline-formula>), low grade glioma (<inline-formula id="IEq25"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=2$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq25.gif"/></alternatives></inline-formula>), renal cell carcinoma (<inline-formula id="IEq26"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=2$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq26.gif"/></alternatives></inline-formula>), Rathke&#x02019;s cyst (<inline-formula id="IEq27"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=1$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq27.gif"/></alternatives></inline-formula>), lipoma (<inline-formula id="IEq28"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=1$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq28.gif"/></alternatives></inline-formula>), and Langerhans cell histiocytosis (<inline-formula id="IEq29"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=1$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq29.gif"/></alternatives></inline-formula>). NOTACP image datasets were obtained from the radiology department at Children&#x02019;s Hospital Colorado (<inline-formula id="IEq30"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=44$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>44</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq30.gif"/></alternatives></inline-formula>) and St. Jude Children&#x02019;s Research Hospital (<inline-formula id="IEq31"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=3$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq31.gif"/></alternatives></inline-formula>). For training, we utilized 23 ACP and 30 NOTACP patient datasets. We extracted three representative images per patient and imaging modality (6 images per patient, 318 images total). The test dataset was comprised of 16 ACP and 17 NOTACP patients, with one representative image selected per patient and imaging modality (66 images total; 33 MRI and 33 CT).</p></sec><sec id="Sec12"><title>Transfer learning and model architecture</title><p id="Par27">Transfer learning was completed by extracting dense one-dimensional feature vectors (image signatures) using models fully trained on the ILSVRC-2012-CLS dataset. These models are publicly available on TensorFlow Hub (Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Pre-trained networks utilized.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Network</th><th align="left">Source</th><th align="left">Feature vector size</th></tr></thead><tbody><tr><td align="left">Inception V1</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/inception_v1/feature_vector/1">https://tfhub.dev/google/imagenet/inception_v1/feature_vector/1</ext-link></td><td align="left">1024</td></tr><tr><td align="left">Inception V2</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/inception_v2/feature_vector/1">https://tfhub.dev/google/imagenet/inception_v2/feature_vector/1</ext-link></td><td align="left">1024</td></tr><tr><td align="left">Inception V3</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1">https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1</ext-link></td><td align="left">2048</td></tr><tr><td align="left">Inception ResNet V2</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/1">https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/1</ext-link></td><td align="left">1536</td></tr><tr><td align="left">ResNet V1 50</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/resnet_v1_50/feature_vector/1">https://tfhub.dev/google/imagenet/resnet_v1_50/feature_vector/1</ext-link></td><td align="left">2048</td></tr><tr><td align="left">ResNet V1 101</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/resnet_v1_101/feature_vector/1">https://tfhub.dev/google/imagenet/resnet_v1_101/feature_vector/1</ext-link></td><td align="left">2048</td></tr><tr><td align="left">ResNet V1 152</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/resnet_v1_152/feature_vector/1">https://tfhub.dev/google/imagenet/resnet_v1_152/feature_vector/1</ext-link></td><td align="left">2048</td></tr><tr><td align="left">ResNet V2 50</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/1">https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/1</ext-link></td><td align="left">2048</td></tr><tr><td align="left">ResNet V2 101</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/1">https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/1</ext-link></td><td align="left">2048</td></tr><tr><td align="left">ResNet V2 152</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/resnet_v2_152/feature_vector/1">https://tfhub.dev/google/imagenet/resnet_v2_152/feature_vector/1</ext-link></td><td align="left">2048</td></tr><tr><td align="left">NASNet-A Large</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/1">https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/1</ext-link></td><td align="left">4032</td></tr><tr><td align="left">PNASNet-5 Large</td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/2">https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/2</ext-link></td><td align="left">4320</td></tr></tbody></table><table-wrap-foot><p>Modules were accessed using the respective URL and standard TensorFlow Hub methods.</p></table-wrap-foot></table-wrap></p><p id="Par28">Resultant image signatures were given as inputs to a single fully-connected layer of the standard form<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{y} = g(\cdot ) = g(f_{T}(w,x)) = g(wx+b) \end{aligned}$$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2020_73278_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq32"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g(\cdot )$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq32.gif"/></alternatives></inline-formula> is the activation function. Prior to activation, input image signatures were transformed via a dropout<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> layer with feature keep probabilities being one of 25%, 50%, 75%, or 100% (i.e. no dropout). We explored the application of several activation functions (softmax, softplus, softsign, ReLU, leaky ReLU, and log softmax) readily available within the TensorFlow library. Model loss was calculated using the canonical categorical cross-entropy<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup> function.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L(y,\hat{y}) = J(w) = -\frac{1}{N}\sum _{n=1}^{N} (y_n \log {\hat{y_n}} + (1 - y_n)\log {(1-\hat{y_n})}) \end{aligned}$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>log</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2020_73278_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where applied, model regularization was implemented using <inline-formula id="IEq33"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{1}$$\end{document}</tex-math><mml:math id="M70"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq33.gif"/></alternatives></inline-formula> or <inline-formula id="IEq34"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{2}$$\end{document}</tex-math><mml:math id="M72"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq34.gif"/></alternatives></inline-formula> (Tikhonov) regularization<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup> using native TensorFlow commands against either model weights, biases, or both.</p><p id="Par29">Each model architecture was then exposed to one of the following optimizers: Gradient Descent, Adam, Adagrad, Adadelta, RMSProp, Momentum, FTRL, Proximal Adagrad, and Proximal Adadelta<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR30">30</xref></sup>. Training batch sizes were one of 2, 5, 10, 20, or 120 images. Training duration ranged from 10 to 125 epochs. Lastly, the learning rate utilized for training was one of 0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05, and 0.1.</p><p id="Par30">We acknowledge the F1 score as being a widely utilized accuracy metric for models that are trained on imbalanced datasets, especially in the computer science field. While our original training dataset is indeed slightly imbalanced towards the NOTACP class, we use a balanced test dataset ubiquitously. In addition, our augmented training datasets are numerically balanced. We chose to evaluate network performance using the clinically commonplace metrics of Receiver Operating Characteristic (ROC) curve and area under the ROC curve (AUC) as they more readily translate meaning to clinical practioners<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>.</p></sec><sec id="Sec13"><title>Genetic algorithm</title><p id="Par31">The genetic algorithm (Algorithm 1) was adapted from a set of publicly available repositories<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>, and it is specifically utilized herein as a meta-parameter optimization solution. Briefly, we randomly select one model feature (e.g., loss function, learning rate, batch size, etc.) for each of the features listed in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>b, this set of features comprises a singular &#x0201c;individual&#x0201d;. For a given generation, we generated 100 of these &#x0201c;individuals&#x0201d;. Each individual is asynchronously processed and the &#x0201c;fitness&#x0201d; of an individual is the AUC value described above. After the full generation has been evaluated, a top fraction is carried over to the next generation. The following generation is created by randomly selecting model features found in the individuals that comprise the top fraction of the the previous generation&#x02014;akin to &#x0201c;offspring&#x0201d; from a &#x0201c;mother&#x0201d; and &#x0201c;father&#x0201d; set. It is worth noting that genetic algorithms are prone to identifying local minima which can lead to biased optimization results, though we have attempted to mitigate this by using random mutation. A visual schematic for this process can be seen in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a.</p><graphic position="anchor" xlink:href="41598_2020_73278_Figa_HTML" id="MO9"/><p id="Par32">The search space of the genetic algorithm included 19,051,200 possible solutions (12 pre-trained networks <inline-formula id="IEq35"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 7$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq35.gif"/></alternatives></inline-formula> learning rates <inline-formula id="IEq36"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 3$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq36.gif"/></alternatives></inline-formula> batch sizes <inline-formula id="IEq37"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 5$$\end{document}</tex-math><mml:math id="M78"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq37.gif"/></alternatives></inline-formula> training epochs <inline-formula id="IEq38"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 15$$\end{document}</tex-math><mml:math id="M80"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq38.gif"/></alternatives></inline-formula> optimizers <inline-formula id="IEq39"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 7$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq39.gif"/></alternatives></inline-formula> activation functions <inline-formula id="IEq40"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 4$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq40.gif"/></alternatives></inline-formula> dropout rates <inline-formula id="IEq41"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 4$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq41.gif"/></alternatives></inline-formula> regularization methods <inline-formula id="IEq42"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 3$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq42.gif"/></alternatives></inline-formula> training datasets <inline-formula id="IEq43"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 3$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq43.gif"/></alternatives></inline-formula> test datasets). Note, that although there are only 9 optimizers explicitly listed in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>B that the proximal optimizers have 4 unique variants (no regularization, l1-regularization, l2-regularization, and l1/l2-regularization; as demarcated by the asterisk in the figure), yielding 15 possible optimizers. The AUC &#x0201c;fitness&#x0201d; value is determined for each network by evaluating on the test mentioned above (<inline-formula id="IEq44"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=66$$\end{document}</tex-math><mml:math id="M92"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>66</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq44.gif"/></alternatives></inline-formula>). On our system we were capable of running 10 networks simultaneously at any given time, and runtime for 10 generations with <inline-formula id="IEq45"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {n}=100$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mtext>n</mml:mtext><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq45.gif"/></alternatives></inline-formula> (i.e., 1000 networks) was approximately 1&#x02013;1.5&#x000a0;days.</p></sec><sec id="Sec14"><title>Image augmentation and synthetic data expansion by TANDA</title><p id="Par33">Standard image augmentation was performed using the Augmentor python library<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Training data was augmented using a pipeline implementing a random distortion (<inline-formula id="IEq46"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {probability}=0.75$$\end{document}</tex-math><mml:math id="M96"><mml:mrow><mml:mtext>probability</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq46.gif"/></alternatives></inline-formula>, <inline-formula id="IEq47"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {grid width}=4$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:mtext>grid width</mml:mtext><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq47.gif"/></alternatives></inline-formula>, <inline-formula id="IEq48"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {grid height}=4$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:mtext>grid height</mml:mtext><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq48.gif"/></alternatives></inline-formula>, <inline-formula id="IEq49"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {magnitude}=8$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:mtext>magnitude</mml:mtext><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq49.gif"/></alternatives></inline-formula>), followed by a random <inline-formula id="IEq50"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$90^{\circ }$$\end{document}</tex-math><mml:math id="M104"><mml:msup><mml:mn>90</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq50.gif"/></alternatives></inline-formula> rotation (<inline-formula id="IEq51"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {probability}=0.75$$\end{document}</tex-math><mml:math id="M106"><mml:mrow><mml:mtext>probability</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq51.gif"/></alternatives></inline-formula>), then a random zoom (<inline-formula id="IEq52"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {probability}=0.5$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:mtext>probability</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq52.gif"/></alternatives></inline-formula>, <inline-formula id="IEq53"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {percentage area}=0.8$$\end{document}</tex-math><mml:math id="M110"><mml:mrow><mml:mtext>percentage area</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq53.gif"/></alternatives></inline-formula>), and finally a random left-right flip (<inline-formula id="IEq54"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {probability}=0.5$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:mtext>probability</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq54.gif"/></alternatives></inline-formula>). CT and MRI data were each sampled using this pipeline for either 100 or 1000 iterations. Test images were sampled using this pipeline, with all probabilities being set to 1.0. Test images were sampled using this pipeline either 10 or 100 times.</p><p id="Par34">Unsupervised GAN-based image generation was performed via minor adaptation to the TANDA python library8 initialized with the following parameters: LSTM-class generator; generator learning rate of <inline-formula id="IEq55"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 10^{-4}$$\end{document}</tex-math><mml:math id="M114"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq55.gif"/></alternatives></inline-formula>; discriminator learning rate of <inline-formula id="IEq56"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 10^{-5}$$\end{document}</tex-math><mml:math id="M116"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq56.gif"/></alternatives></inline-formula>; gamma equal to 0.5; one mean-squared-error (MSE) layer; MSE-term coefficient of <inline-formula id="IEq57"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 10^{-3}$$\end{document}</tex-math><mml:math id="M118"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_73278_Article_IEq57.gif"/></alternatives></inline-formula>; transformation sequence length of 10; no per-image standardization; trained using a batch size of 5 and for a duration of 5 epochs. We sought to extract the generated images as JPEG files for visualization, as opposed to direct import into an end-classifier. 1000 ACP and 1000 NOTACP synthetic images were generated for both CT and MRI modalities.</p></sec><sec id="Sec15"><title>Computational hardware and software</title><p id="Par35">All computational programs were performed on a 64-bit RedHat Enterprise Linux HPC running CentOS 7.4.1708. Python based programs were executed in a virtual environment containing Python 3.6 with the following modules: Augmentor (v 0.2.2)<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, Matplotlib (v 2.2.2)<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, Numpy (v 1.14.15)<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, Pandas (v 0.23.3)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, Ray (v 0.6.4)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, Sci-kit Image (v 0.14.0)<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, TensorFlow (v 1.12.0)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, and TensorFlow Hub (v 0.2.0)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>.</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The authors wish to express their gratitude for study coordinators Anastasia Arynchna (University of Alabama Birmingham), Hannah Goldstein (Columbia University), Stephen Gannon (Vanderbilt University), Corrine Gardner (Washington University St. Louis), Anthony Bet (Stanford University), Nassima Addour (McGill Univeristy), Kari Bollerman (Miami Children&#x02019;s Hospital), Alyson Hignight (Cornell Univeristy), Robyn Ryans (Children&#x02019;s Mercy Hospital), Kris Laurence (Children&#x02019;s Mercy Hospital), Lisa Tetreault (Johns Hopkins All Children&#x02019;s Hospital), Jennifer Spinelli (Orlando Health), Kaitlin Hardy (Children&#x02019;s National Medical Center), Sabrina Malik (Children&#x02019;s National Medical Center), and Brandy Vaughn (Lebonheur Children&#x02019;s Research Hospital) for their assistance in making this study possible. The authors also wish to thank the University of Colorado Comprehensive Cancer Center for funding that supported this work (P30CA046934).</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Project Conception: EP, TCH; Project Design: EP, TCH; Software Generation: EP; Data Acquisition: RW, NS, DM, SS, PK, RCEA, TNN, GG, MS, JMJ, EMJ, DDL, AS, AMD, JC, LK, KG, RN, RD, ETK, GJ, MHH; Data Analysis: EP, KJ; Data Interpretation: EP, KJ, AMD, NF, TCH; Manuscript Composition: EP, RW, TCH; Manuscript Revisions: KJ, AMD, NF, TCH;</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The dataset analyzed during the current study is available from the corresponding author on reasonable request.</p></notes><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par40">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><source>Deep Learning</source><year>2016</year><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulshan</surname><given-names>V</given-names></name><etal/></person-group><article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title><source>JAMA</source><year>2016</year><volume>316</volume><fpage>2402</fpage><lpage>2410</lpage><pub-id pub-id-type="doi">10.1001/jama.2016.17216</pub-id><pub-id pub-id-type="pmid">27898976</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Russakovsky, O. <italic>et al.</italic> Imagenet large scale visual recognition challenge. <italic>CoRR</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1409.0575">arXiv:abs/1409.0575</ext-link> (2014).</mixed-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title><source>J. Mach. Learn. Res.</source><year>2014</year><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>J</given-names></name><etal/></person-group><article-title>Transfer learning using computational intelligence: a survey</article-title><source>Knowl. Based Syst.</source><year>2015</year><volume>80</volume><fpage>14</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.knosys.2015.01.010</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Bengio, Y., Courville, A. C. &#x00026; Vincent, P. Unsupervised feature learning and deep learning: a review and new perspectives. <italic>CoRR</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1206.5538">arXiv:abs/1206.5538</ext-link> (2012).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Ratner, A. J., Ehrenberg, H. R., Hussain, Z., Dunnmon, J. &#x00026; R&#x000e9;, C. Learning to compose domain-specific transformations for data augmentation. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1709.01643">arXiv:1709.01643</ext-link> (2017).</mixed-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>P</given-names></name><name><surname>Jalali</surname><given-names>R</given-names></name></person-group><article-title>Long-term survivors of childhood brain tumors: impact on general health and quality of life</article-title><source>Curr. Neurol. Neurosci. Rep.</source><year>2017</year><volume>17</volume><fpage>99</fpage><pub-id pub-id-type="doi">10.1007/s11910-017-0808-0</pub-id><pub-id pub-id-type="pmid">29119343</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>GA</given-names></name><etal/></person-group><article-title>Diagnostic accuracy of neuroimaging in pediatric optic chiasm/sellar/suprasellar tumors</article-title><source>Pediatr. Blood Cancer</source><year>2019</year><volume>66</volume><fpage>e27680</fpage><pub-id pub-id-type="doi">10.1002/pbc.27680</pub-id><pub-id pub-id-type="pmid">30848081</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Abadi, M. <italic>et al.</italic> Tensorflow: large-scale machine learning on heterogeneous distributed systems. <italic>CoRR</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1603.04467">arXiv:abs/1603.04467</ext-link> (2016).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. Deep residual learning for image recognition. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1512.03385">arXiv:abs/1512.03385</ext-link> (2015).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Ioffe, S. &#x00026; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1502.03167">arXiv:abs/1502.03167</ext-link> (2015).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Liu, C. <italic>et al.</italic> Progressive neural architecture search. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1712.00559">arXiv:abs/1712.00559</ext-link> (2017).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Szegedy, C., Ioffe, S. &#x00026; Vanhoucke, V. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1602.07261">arXiv:abs/1602.07261</ext-link> (2016).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. Identity mappings in deep residual networks. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1603.05027">arXiv:abs/1603.05027</ext-link> (2016).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &#x00026; Wojna, Z. Rethinking the inception architecture for computer vision. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1512.00567">arXiv:abs/1512.00567</ext-link> (2015).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Perez, L. &#x00026; Wang, J. The effectiveness of data augmentation in image classification using deep learning. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1712.04621">arXiv:abs/1712.04621</ext-link> (2017).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Ciresan, D. C., Meier, U., Gambardella, L. M. &#x00026; Schmidhuber, J. Deep big simple neural nets excel on handwritten digit recognition. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1003.0358">arXiv:abs/1003.0358</ext-link> (2010).</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>KP</given-names></name></person-group><source>Machine Learning: A Probabilistic Perspective</source><year>2013</year><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Ng, A. Y. Feature selection, l1 vs. l2 regularization, and rotational invariance. In <italic>Proceedings of the Twenty-First International Conference on Machine Learning</italic>, ICML &#x02019;04, 78, 10.1145/1015330.1015435 (Association for Computing Machinery, New York, NY, USA, 2004).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Regularization for sparsity: L1 regularization. <ext-link ext-link-type="uri" xlink:href="https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization">https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization</ext-link>. Accessed 15 June 2020</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Kingma, D. P. &#x00026; Ba, J. Adam: a method for stochastic optimization (2014). Cite <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980Comment">arxiv:1412.6980Comment</ext-link>: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.</mixed-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cauchy</surname><given-names>MA</given-names></name></person-group><article-title>Methode generale pour la resolution des systemes d&#x02019;equations simultanees</article-title><source>C. R. des seances de l'academie des Sci.</source><year>1847</year><volume>81</volume><fpage>536</fpage><lpage>538</lpage></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchi</surname><given-names>J</given-names></name><name><surname>Hazan</surname><given-names>E</given-names></name><name><surname>Singer</surname><given-names>Y</given-names></name></person-group><article-title>Adaptive subgradient methods for online learning and stochastic optimization</article-title><source>J. Mach. Learn. Res.</source><year>2011</year><volume>12</volume><fpage>2121</fpage><lpage>2159</lpage></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Duchi, J. &#x00026; Singer, Y. Proximal and first-order methods for convex optimization. <ext-link ext-link-type="uri" xlink:href="https://ppasupat.github.io/a9online/uploads/proximal_notes">https://ppasupat.github.io/a9online/uploads/proximal_notes</ext-link>. Accessed 15 June 2020</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Zeiler, M. D. ADADELTA: an adaptive learning rate method. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1212.5701">arXiv:abs/1212.5701</ext-link> (2012).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">McMahan, H. B. <italic>et al.</italic> Ad click prediction: a view from the trenches. In <italic>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, KDD &#x02019;13, 1222&#x02013;1230, 10.1145/2487575.2488200 (Association for Computing Machinery, New York, NY, USA, 2013).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Sutskever, I., Martens, J., Dahl, G. &#x00026; Hinton, G. On the importance of initialization and momentum in deep learning. In <italic>Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28</italic>, ICML&#x02019;13, III&#x02013;1139&#x02013;III&#x02013;1147 (JMLR.org, 2013).</mixed-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Duchi</surname><given-names>JC</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Schuurmans</surname><given-names>D</given-names></name><name><surname>Lafferty</surname><given-names>JD</given-names></name><name><surname>Williams</surname><given-names>CKI</given-names></name><name><surname>Culotta</surname><given-names>A</given-names></name></person-group><article-title>Efficient learning using forward-backward splitting</article-title><source>Advances in Neural Information Processing Systems</source><year>2009</year><publisher-loc>Red Hook</publisher-loc><publisher-name>Curran Associates Inc</publisher-name><fpage>495</fpage><lpage>503</lpage></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanley</surname><given-names>JA</given-names></name><name><surname>McNeil</surname><given-names>BJ</given-names></name></person-group><article-title>The meaning and use of the area under a receiver operating characteristic (roc) curve</article-title><source>Radiology</source><year>1982</year><volume>143</volume><fpage>29</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1148/radiology.143.1.7063747</pub-id><pub-id pub-id-type="pmid">7063747</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>AP</given-names></name></person-group><article-title>The use of the area under the roc curve in the evaluation of machine learning algorithms</article-title><source>Pattern Recognit.</source><year>1997</year><volume>30</volume><fpage>1145</fpage><lpage>1159</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(96)00142-2</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Harvey, M. Let&#x02019;s evolve a neural network with a genetic algorithm. <ext-link ext-link-type="uri" xlink:href="https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164">https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164</ext-link>. Accessed 15 June 2020.</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Larson, W. Genetic algorithms: cool name and damn simple. <ext-link ext-link-type="uri" xlink:href="https://lethain.com/genetic-algorithms-cool-name-damn-simple/">https://lethain.com/genetic-algorithms-cool-name-damn-simple/</ext-link>. Accessed 15 June 2020.</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Bloice, M. D., Stocker, C. &#x00026; Holzinger, A. Augmentor: an image augmentation library for machine learning. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1708.04680">arXiv:abs/1708.04680</ext-link> (2017).</mixed-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><article-title>Matplotlib: A 2d graphics environment</article-title><source>Comput. Sci. Eng.</source><year>2007</year><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><article-title>Python for scientific computing</article-title><source>Comput. Sci. Eng.</source><year>2007</year><volume>9</volume><fpage>10</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.58</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">McKinney, W. <italic>et al.</italic> Data structures for statistical computing in python. In <italic>Proceedings of the 9th Python in Science Conference</italic>, vol. 445, 51&#x02013;56 (Austin, TX, 2010).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Moritz, P. <italic>et al.</italic> Ray: a distributed framework for emerging AI applications. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/abs/1712.05889">arXiv:abs/1712.05889</ext-link> (2017).</mixed-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><etal/></person-group><article-title>scikit-image: image processing in Python</article-title><source>PeerJ</source><year>2014</year><volume>2</volume><fpage>e453</fpage><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref></ref-list></back></article>